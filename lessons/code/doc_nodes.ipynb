{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document/Nodes\n",
    "\n",
    "- Document: \n",
    "   \n",
    "   A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be constructed manually, or created automatically via our data loaders\n",
    "\n",
    "- Node:\n",
    "   \n",
    "   A Node represents a \"chunk\" of a source Document, whether that is a text chunk, an image, or other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By built-in example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='9163241c-dea3-4312-80c7-7f73e1f3e2ac', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\nThey are pre-trained on large amounts of publicly available data.\\nHow do we best augment LLMs with our own private data?\\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\nProposed Solution\\nThat\\'s where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps. It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\nProvides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\nProvides an advanced retrieval/query interface over your data:\\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\\nAllows easy integrations with your outer application framework\\n(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\\nLlamaIndex provides tools for both beginner users and advanced users.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and\\nquery their data in 5 lines of code. Our lower-level APIs allow advanced users to\\ncustomize and extend any module (data connectors, indices, retrievers, query engines,\\nreranking modules), to fit their needs.\\n', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = Document.example()\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\nThey are pre-trained on large amounts of publicly available data.\\nHow do we best augment LLMs with our own private data?\\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\nProposed Solution\\nThat\\'s where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps. It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\nProvides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\nProvides an advanced retrieval/query interface over your data:\\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\\nAllows easy integrations with your outer application framework\\n(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\\nLlamaIndex provides tools for both beginner users and advanced users.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and\\nquery their data in 5 lines of code. Our lower-level APIs allow advanced users to\\ncustomize and extend any module (data connectors, indices, retrievers, query engines,\\nreranking modules), to fit their needs.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = example.get_content()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Context\n",
       "LLMs are a phenomenal piece of technology for knowledge generation and reasoning.\n",
       "They are pre-trained on large amounts of publicly available data.\n",
       "How do we best augment LLMs with our own private data?\n",
       "We need a comprehensive toolkit to help perform this data augmentation for LLMs.\n",
       "\n",
       "Proposed Solution\n",
       "That's where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\n",
       "you build LLM  apps. It provides the following tools:\n",
       "\n",
       "Offers data connectors to ingest your existing data sources and data formats\n",
       "(APIs, PDFs, docs, SQL, etc.)\n",
       "Provides ways to structure your data (indices, graphs) so that this data can be\n",
       "easily used with LLMs.\n",
       "Provides an advanced retrieval/query interface over your data:\n",
       "Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
       "Allows easy integrations with your outer application framework\n",
       "(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\n",
       "LlamaIndex provides tools for both beginner users and advanced users.\n",
       "Our high-level API allows beginner users to use LlamaIndex to ingest and\n",
       "query their data in 5 lines of code. Our lower-level APIs allow advanced users to\n",
       "customize and extend any module (data connectors, indices, retrievers, query engines,\n",
       "reranking modules), to fit their needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manaully create document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(text=\"Â∫≠Èô¢Ê∑±Ê∑±Ê∑±ÂπæË®±\", metadata={\"file_name\": \"mathbook.txt\"}),\n",
    "    Document(text=\"ÂìàÂõâ‰Ω†Â•ΩÂóé\", metadata={\"file_name\": \"song.md\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='98e0f3e6-30ac-412e-bb28-68d535b0a7c6', embedding=None, metadata={'file_name': 'mathbook.txt'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Â∫≠Èô¢Ê∑±Ê∑±Ê∑±ÂπæË®±', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Â∫≠Èô¢Ê∑±Ê∑±Ê∑±ÂπæË®±'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].get_content()  # Default `get_content` function does not include metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file_name: mathbook.txt\\n\\nÂ∫≠Èô¢Ê∑±Ê∑±Ê∑±ÂπæË®±'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the content with metadata\n",
    "from llama_index.core.schema import MetadataMode\n",
    "documents[0].get_content(metadata_mode=MetadataMode.ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â∫≠Èô¢Ê∑±Ê∑±Ê∑±ÂπæË®±\n",
      "Â∫≠Èô¢Ê∑±Ê∑±Ê∑±ÂπæË®±\n"
     ]
    }
   ],
   "source": [
    "# set excludes metadata keys for both embed and llm\n",
    "documents[0].excluded_embed_metadata_keys=[\"file_name\"]\n",
    "documents[0].excluded_llm_metadata_keys=[\"file_name\"]\n",
    "\n",
    "# Test the content during embed and llm\n",
    "print(documents[0].get_content(metadata_mode=MetadataMode.EMBED))\n",
    "print(documents[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Parser (JSON data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(input_dir=\"../data/json/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import JSONNodeParser\n",
    "json_parser = JSONNodeParser()\n",
    "nodes = json_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title „ÄêÂè∞ÁÅ£Èô∑ÂÖ•Â∞∑Â∞¨ËôïÂ¢É„ÄëÂçäÂ∞éÈ´îËÄóÈõªÈáèÁãÇÈ£ÜÂèàÂæóÊ∑®Èõ∂ËΩâÂûãÔºåÊ†∏ÈõªÊúÉÊòØÊúÄ‰Ω≥Ëß£Ôºü\\ncontent Âè∞ÁÅ£‰ºÅÊ•≠ÊªøË∂≥‰∫ÜÂÖ®ÁêÉÈ´òÈÅî 68% ÁöÑÊô∂ÁâáË£ΩÈÄ†‰æõÊáâÔºåÈö®‰πãËÄå‰æÜÁöÑÊòØÂ§ßÈáèËÉΩÊ∫êÈúÄÊ±Ç‚Äî‚ÄîÊ†πÊìöÁ∂†Ëâ≤ÂíåÂπ≥ÁµÑÁπîÁöÑÈ†êÊ∏¨Ôºå2030 Âπ¥Âè∞ÁÅ£ÂçäÂ∞éÈ´îË£ΩÈÄ†Áî¢Ê•≠‰πãËÄóÈõªÈáèÔºåÂ∞áÊúÉÁõ∏Áï∂Êñº 2021 Âπ¥Á¥êË•øËò≠ÂÖ®Â≥∂ÁöÑËÄóÈõªÈáèÁöÑÂÖ©ÂÄçÔºåÂÖ∂‰∏≠Â∞áÊúâ 82% ÁöÑÈúÄÊ±Ç‰æÜËá™Âè∞Á©çÈõª„ÄÇ\\nÂè∞ÁÅ£ÁÇ∫‰ΩïÈô∑ÂÖ•ËÉΩÊ∫êÂç±Ê©ü„ÄÅËàáÊ∑®Èõ∂ÁõÆÊ®ôÈÅôÈÅôÁÑ°ÊúüÔºü\\nAI ÊôÇ‰ª£Áî®ÈõªÊøÄÂ¢ûÔºåÂè∞ÁÅ£ÂçªÂú®Ê≠§ÊôÇÈù¢Ëá®ÁâΩÊ∂âÂà∞ÂúãÂÆâ„ÄÅÊ∞£ÂÄô„ÄÅÊîøÊ≤ªÊåëÊà∞ÁöÑËÉΩÊ∫êÂõ∞Â¢ÉÔºöÂè∞ÁÅ£ÂÖ®Â≥∂ÊúâÈ´òÈÅî 9 ÊàêÁöÑÁî®ÈõªÈáèÈù†ÁöÑÊòØÈÄ≤Âè£Áü≥ÂåñÁáÉÊñôÔºõÂÖ©Â≤∏ÊÉÖÂã¢‰πüÊåÅÁ∫åÁ∑äÁπÉÔºåÂè∞ÁÅ£ÂøÖÈ†àÈù¢Â∞ç‰æÜËá™‰∏≠ÂúãÁöÑÁ∂ìÊøüÂ∞ÅÈéñ„ÄÅÂúãÈöõÂ≠§Á´ãÔºåÁîöÊàñÊ≠¶ÂäõÂÖ•‰æµÂ®ÅËÑÖÔºõËÄåÂá∫ÊñºÊîøÊ≤ªËÄÉÈáèÔºåÂü∑ÊîøÈª®‰∏ªÂºµÂú® 2025 Âπ¥ÂâçÂëäÂà•Ê†∏ÈõªÔºåÊâìÈÄ†„ÄåÈùûÊ†∏ÂÆ∂Âúí„ÄçÔºå‰∏¶Âú®Âêå‰∏ÄÂπ¥ÈÅîÊàê„ÄåÁáÉÁÖ§ÁôºÈõª 30%„ÄÅÂ§©ÁÑ∂Ê∞£ÁôºÈõª 50%„ÄÅÂÜçÁîüËÉΩÊ∫ê 20%„ÄçÁöÑËÉΩÊ∫êÁµêÊßãÁõÆÊ®ô„ÄÇ\\nÂÜçËÄÖÔºåÊîøÂ∫úÈÇÑË®ÇÁ´ãÈáéÂøÉÂãÉÂãÉÁöÑÊΩîÊ∑®ËÉΩÊ∫êÁõÆÊ®ôÔºåÂåÖÂê´ÈÅµÂæ™Â∑¥ÈªéÂçîË≠∞Ôºå2030 Âπ¥Âè∞ÁÅ£Â∞áÊ∏õÁ¢≥ 23%Ôºå‰∏¶Âú® 2050 Âπ¥ÂâçÈÅîÊàêÊ∑®Èõ∂Á¢≥ÊéíÔºõÁßÅÈÉ®ÈñÄÊñπÈù¢ÔºåÂåÖÂê´Âè∞Á©çÈõªÂú®ÂÖßÁöÑÂ§öÂÆ∂Â§ßÂª†ÔºåÈÉΩÁ∞ΩÁΩ≤‰∫Ü RE100 ÂÖ®ÁêÉÂÜçÁîüËÉΩÊ∫êÂÄ°Ë≠∞ÔºåÊâøË´æÊñº 2050 Âπ¥ÂâçÊé°Áî® 100ÔºÖ Á∂†Èõª„ÄÇ\\nÁõÆÂâçÁúã‰æÜÔºåÊ≤íÊúâ‰ªª‰Ωï‰∏ÄÁµ≤ÈÅîÊ®ôË∑°Ë±°ÔºåÁèæÂØ¶ÂíåÁêÜÊÉ≥‰πãÈñìÂ∞öÂ≠òÂú®ËëóÈõ£‰ª•Ë∑®Ë∂äÁöÑÈ¥ªÊ∫ù„ÄÇ\\nÂè∞ÁÅ£ÁèæË°åËÉΩÊ∫êÁµêÊßãÂçÅÂàÜËÑÜÂº±ÔºåÊ•µÂ∫¶‰ª∞Ë≥¥ÈÄ≤Âè£\\nÊ†πÊìöÁ∂ìÊøüÈÉ®Áµ±Ë®àÔºåÂéªÂπ¥Âè∞ÁÅ£Êúâ 83% ÁöÑÁî®ÈõªÈúÄÊ±Ç‰ª∞Ë≥¥Áü≥ÂåñÁáÉÊñôÔºåÂÖ∂‰∏≠ÁÖ§ÁÇ≠ÁôºÈõª‰Ωî 42%„ÄÅÂ§©ÁÑ∂Ê∞£‰Ωî 40%„ÄÅÁü≥Ê≤π‰Ωî 1%ÔºåÂè¶Â§ñÊ†∏ËÉΩ‰Ωî 6%ÔºåËÄåÂ§™ÈôΩËÉΩ„ÄÅÈ¢®Âäõ„ÄÅÊ∞¥Âäõ„ÄÅÁîüË≥™ËÉΩÁôºÈõªÂä†Á∏ΩËµ∑‰æÜ‰πüÊâç‰Ωî 10%„ÄÇ\\nÈÄôÊ®£ÁöÑËÉΩÊ∫ê‰æõÊáâÁ≥ªÁµ±Ê•µ‰∏çÁ©©ÂÆöÔºåÂõ†ÁÇ∫ÁáÉÊñôÈÄ≤Âè£Èö®ÊôÇÈù¢Ëá®Âà∞ÂúãÈöõÂÉπÊ†ºÊ≥¢ÂãïÊàñ‰∏≠ÂúãÂ∞ÅÈéñÁöÑÈ¢®Èö™ÔºõÂç≥‰æøÂè∞ÁÅ£ÊîøÂ∫úËÉΩÂá∫ÊâãË™øÊï¥ÈõªÂÉπÔºåÂçª‰πüÈÄ†ÊàêÂè∞ÈõªÂÇµÂè∞È´òÁØâÔºåËÄå‰∏ÄÊó¶‰∏≠ÂúãÊµ∑ËªçÂ∞ÅÈéñÂè∞ÁÅ£Êµ∑Â≥ΩÔºåÂè∞ÁÅ£Â≥∂Á¥ÑÂè™Êúâ 6 ÈÄ±ÁöÑÁÖ§ÁÇ≠ÂÑ≤ÂÇôÈáèÔºå‰ª•ÂèäÁ¥Ñ 1 ÈÄ±ÁöÑÊ∂≤ÂåñÂ§©ÁÑ∂Ê∞£ÂÑ≤ÂÇôÈáè„ÄÇ\\nÁ∏±‰Ωø‰∏äËø∞È¢®Èö™ÈÉΩÂ∞öÊú™ÁôºÁîüÔºå‰ªäÂπ¥Âè∞ÁÅ£ÁôºÈõªÁöÑÂÇôËΩâÂÆπÈáèÂ∑≤Â§öÊ¨°ÊéâÂà∞ 5%ÔºàÁêÜÊÉ≥ÁöÑÁ≥ªÁµ±ÂÇôËΩâÂÆπÈáèÁÇ∫ 25%ÔºâÔºå‰∏îÂú®ÈÅéÂéª 8 Âπ¥ÂÖßÔºåÂ∞±ÁôºÁîüÈÅé 4 Ê¨°Â§ßË¶èÊ®°Ë∑≥ÈõªÔºåÈôêÈõªÊÉÖÊ≥Å‰πü‰∏çÂ∞ëË¶ãÔºåÂú®Âú®È°ØÁ§∫Âá∫Âè∞ÁÅ£‰æõÈõªÂõ∞Â¢É‰∫üÂæÖËß£Ê±∫„ÄÇ\\nÂÜçÁîüËÉΩÊ∫êÁÇ∫‰ΩïÈÄ≤Â±ïÁ∑©ÊÖ¢ÔºüÂ∞àÂÆ∂Ë≠¶ÂëäÂ§ñË≥áÂá∫Ëµ∞ÂèØËÉΩÊÄß\\nÁî±ÊñºÂè∞ÁÅ£ÂúãÂúüÈù¢Á©çÂ∞è‰∏îÂ±±Âú∞Â§öÔºåÂ§™ÈôΩËÉΩÁôºÈõªÈù¢Ëá®ÂúüÂú∞ÂèñÂæó‰∏çÊòìÁöÑÈôêÂà∂„ÄÇÈõ¢Â≤∏È¢®ÈõªÈõñÂ§ßÊúâÊΩõÂäõÔºå‰ΩÜËøëÂπ¥ÂèóÂà∞ÊîøÂ∫úÊ¨≤Êâ∂Ê§çÂú®Âú∞Áî¢Ê•≠ÁôºÂ±ïÁöÑÊîøÁ≠ñÈôêÂà∂ÔºåÂè™ËÉΩÈÅãÁî® MIT Áî¢ÂìÅÂíåËÅòÁî®Âè∞ÁÅ£ÂãûÂ∑•ÔºåÁ§ôÊñºÊäÄË°ìËêΩÂ∑ÆÔºåÈÄ†Êàê‰∏çÂøÖË¶ÅÁöÑÊàêÊú¨ÊîØÂá∫ËàáÂ∑•Á®ãÂª∂ÂÆïÔºõÂè¶‰∏ÄÊñπÈù¢ÔºåÊ∏ØÂè£ËÖπÂú∞Â∞è„ÄÅÈ¢®ÂäõÁôºÈõªÊ©üÁõ¥Êé•‰ΩçÊñº‰∏≠ÂúãÈ£õÂΩàÂ∞ÑÁ®ãÁØÑÂúçÂÖßÔºåÊµ∑Â∫ïÈõªÁ∫úÂú®Ëªç‰∫ãË°ùÁ™Å‰∏≠È´òÂ∫¶ËÑÜÂº±ÔºåÈÄô‰∫õÈÉΩÊßãÊàêÂè∞ÁÅ£ÂÜçÁîüËÉΩÊ∫ê‰πãÁôºÂ±ïÂõ∞Â¢É„ÄÇ\\nNicholas Chen ÊòØ‰∏Ä‰ΩçÈï∑ÊúüÁ†îÁ©∂Âè∞ÁÅ£Ê∞£ÂÄôËàáËÉΩÊ∫êÊîøÁ≠ñÁöÑÂæãÂ∏´Ôºå‰ªñË™çÁÇ∫Âè∞ÁÅ£ÂøÖÈ†àÁÇ∫ÊΩîÊ∑®ËÉΩÊ∫êÁöÑÁôºÂ±ïË∑ØÂæëÂÅöÂá∫Â§ßÂπÖË™øÊï¥ÔºåÂê¶ÂâáÂèØËÉΩÂ∞áÈù¢Ëá®Â§ñË≥áÂá∫Ëµ∞ÁöÑÂç±Ê©ü‚Äî‚ÄîÊúâË∂ä‰æÜË∂äÂ§ö‰ºÅÊ•≠ÈúÄË¶ÅÊ∑®Èõ∂ÊéíÊîæÁöÑÁîüÁî¢Áí∞Â¢ÉÔºå‰ª•ÊªøË∂≥Â¶Ç‰∫ûÈ¶¨ÈÅú„ÄÅMeta„ÄÅGoogle Á≠âÂêà‰ΩúÂ§•‰º¥ÁöÑÊ∑®Èõ∂ÊâøË´æÔºåÂêåÊôÇÈÅøÂÖçÁ¢≥ÊéíÁõ∏ÈóúÁöÑË≤øÊòìÂ£ÅÂ£òÔºå‰æãÂ¶ÇÊ≠êÁõüÁöÑÁ¢≥ÈÇäÂ¢ÉË™øÊï¥Ê©üÂà∂ÔºàCBAMÔºâ„ÄÇÔºàÁ∑®ÊåâÔºöGoogle„ÄÅAWS„ÄÅÂæÆËªüÁõÆÂâçÁöÜÊúâÂú®Âè∞ÁÅ£Ë®≠Á´ãË≥áÊñô‰∏≠ÂøÉ„ÄÇÔºâ\\nÊ†∏ÈõªÈáçÂïüÂÜçÊéÄË®éË´ñÔºå‰ΩÜÊòØÂê¶ÁÇ∫ÊúÄ‰Ω≥Ëß£Ôºü\\nÈáùÂ∞çÁï∂ÂâçÁöÑËÉΩÊ∫êÂç±Ê©üÔºåÁ∂ìÊøüÈÉ®ËÉΩÊ∫êÁΩ≤ÂâØÁΩ≤Èï∑Âê≥ÂøóÂÅâË°®Á§∫Ôºö„ÄåÁ´ãÊ≥ïÈô¢Ë£°ÁöÑËæØË´ñÈÇÑÂú®ÊåÅÁ∫åÔºåÂõ†ÁÇ∫Á§æÊúÉÂ§ßÁúæÊó¢Ê∏¥ÊúõÈùûÊ†∏ÂÆ∂ÂúíË¢´ÈÄêÊ≠•ÂØ¶ÁèæÔºåÂèàÊÉ≥ÁúãÂà∞Á¢≥ÊéíÈáèÊ∏õÂ∞ëÔºåÂõ†Ê≠§ÈñãÂßãÊúâ‰∫∫Êé¢Ë®éÁï∂ÂÖ∂‰ªñÊ¢ù‰ª∂ÈΩäÂÇôÔºåÈóúÈñâÁöÑÊ†∏ÈõªÂª†ÊòØÂê¶ÊúâÈáçÂïüÁöÑÂèØËÉΩÊÄß„ÄÇ„Äç\\nÁÇ∫Ê≠§ÔºåNicholas Chen ÊåáÂá∫„ÄåÂîØÊúâÊ†∏ËÉΩÊâçÊòØÂèØÊì¥Â±ïÁöÑÂÜçÁîüËÉΩÊ∫ê„ÄçÔºåÁßëÊäÄÈÉ®È°ßÂïè Peter Kurz ÂâáÈÄ≤‰∏ÄÊ≠•ÊåáÂá∫ÔºåÂè∞ÁÅ£ÁöÑËÉΩÊ∫êÂõ∞Â¢ÉÈúÄË¶ÅË∑≥ËÑ´Ê°ÜÊû∂ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÊúâÈëëÊñºÊµ∑Â∫ïÈõªÁ∫úÁöÑËÑÜÂº±ÊÄßÔºå‰∏îÊ†∏ËÉΩÈõªÂª†ÂÆπÊòìÂèóÂú∞ÈúáÂ®ÅËÑÖÔºåÂè¶‰∏ÄÈ†ÖÂü∑ÊîøÈª®ÂèØËÉΩËÄÉÊÖÆÁöÑÂÅöÊ≥ïÔºåÊòØËÉΩÈõ¢Â≤∏ÈÅã‰ΩúÁöÑÂ∞èÂûãÊ®°ÁµÑÂåñÁöÑÊ†∏ÂèçÊáâÁàê„ÄÇ\\nË©±ÈõñÂ¶ÇÊ≠§Ôºå‰∏ÄÈ†ÖÂè≤‰∏π‰ΩõÂ§ßÂ≠∏ÁöÑÁ†îÁ©∂ÁôºÁèæÔºåÂ∞èÂûãÊ†∏ÂèçÊáâÁàêÂ∞á‰ΩøÊ†∏Âª¢ÊñôËôïÁΩÆÊõ¥Âä†Ê£òÊâã„ÄÇË©≤Á†îÁ©∂Ê∏¨Ë©¶‰∫Ü‰∏âÂ∫ßÂèçÊáâÁàêÔºåÁôºÁèæÊâÄÈÄ†ÊàêÁöÑÊîæÂ∞ÑÁâ©Ë≥™„ÄåÈàΩ„ÄçÔºåÂÖ∂‰∏ÄËê¨Âπ¥ÂæåÁöÑÊîæÂ∞ÑÊÄßÂ∞áÊØîÂÇ≥Áµ±Ê†∏ÈõªÂª†È´ò 50% ‰ª•‰∏ä„ÄÇ\\nÂè∞ÁÅ£‰ΩúÁÇ∫ÂÖ®ÁêÉÊúÄÂ§ßÁöÑÂÖàÈÄ≤Êô∂ÁâáË£ΩÈÄ†ÂúãÔºåÂúãÂÖßÈõªÂäõ‰æõ‰∏çÊáâÊ±ÇÔºåÈ´òÂ∫¶‰ª∞Ë≥¥ÈÄ≤Âè£ÁáÉÊñôÔºåÂç≥Â∞áÈóúÈñâÊúÄÂæå‰∏ÄÂ∫ßÊ†∏ÈõªÂª†ÔºåÂÜçÁîüËÉΩÊ∫êÁôºÂ±ïÂèàÈÅ≤Á∑©ÔºåÊúùÈáéÂ¶Ç‰ΩïÂçîÂïÜÂá∫Â¶•ÂñÑÁöÑÊáâÂ∞çÊé™ÊñΩÔºåÁ¢∫‰øùËÉΩÊ∫ê‰æõÊáâÁ©©ÂÆöÔºåÂ∑≤ÊòØÂàª‰∏çÂÆπÁ∑©ÁöÑË≠∞È°å„ÄÇ\\n„ÄêÊé®Ëñ¶Èñ±ËÆÄ„Äë\\n‚óÜ „ÄêÁ¢≥Ë≤ª‰∏äË∑Ø„ÄëÂÖ®ÂúãÂæµÊî∂Â∞çË±°ÈÅî 500 ÂÆ∂ÔºÅÊîøÂ∫úÊâìÁÆóÊÄéÈ∫º‰ΩøÁî®Êî∂Âà∞ÁöÑ 60 ÂÑÑÔºü\\n‚óÜ ÊîπÂñÑÂ∑•Ê•≠ËÉΩÊ∫êÊïàÁéáÂæû‰ΩïÂÅöËµ∑ÔºüÂ†±ÂëäÊè≠ 3 Â§ßË°åÂãïÊØèÂπ¥ÂÖ®ÁêÉ‰∏ÄËµ∑ÁúÅ‰∏ã 2 ÂÖÜÁæéÂÖÉ\\n‚óÜ WEF „ÄåÁáàÂ°îÂ∑•Âª†„ÄçÂÖ¨Â∏ÉÔºÅÈ¥ªÊµ∑Êè≠Êãø‰∏ã 2 Â∫ßÁáàÂ°îÂ∑•Âª†ËÉåÂæåÈóúÈçµÊäÄË°ì\\nÔºäÊú¨ÊñáÈñãÊîæÂêà‰ΩúÂ§•‰º¥ËΩâËºâÔºåÂèÉËÄÉË≥áÊñôÔºö„ÄäYale Environment 360„Äã„ÄÅStanfordÔºåÈ¶ñÂúñ‰æÜÊ∫êÔºöShutterstock„ÄÇ\\nÔºàË≤¨‰ªªÁ∑®ËºØÔºöÂªñÁ¥π‰º∂Ôºâ'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].get_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Parser (HTML Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(input_dir=\"../data/html/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "html_parser = HTMLNodeParser()\n",
    "# DEFAULT_TAGS = [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]\n",
    "nodes = html_parser.get_nodes_from_documents(documents)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to My Webpage\n",
      "This is a simple HTML example with some basic styling.\n",
      "Visit\n",
      "Example.com\n",
      "to\n",
      "      learn more.\n",
      "HTML\n",
      "CSS\n",
      "JavaScript\n"
     ]
    }
   ],
   "source": [
    "for each in nodes:\n",
    "    print(each.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Parser (Markdown data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(input_dir=\"../data/markdown/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "md_parser = MarkdownNodeParser()\n",
    "nodes = md_parser.get_nodes_from_documents(documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üêç Python Á∞°ÂñÆË™ûÊ≥ïÁ≠ÜË®ò\\n\\n1. ËÆäÊï∏ËàáË≥áÊñôÂûãÂà•\\n\\n   Python ÊòØ‰∏ÄÂÄãÂãïÊÖãÂûãÂà•Ë™ûË®ÄÔºåÁÑ°ÈúÄÊòéÁ¢∫ËÅ≤ÊòéËÆäÊï∏ÂûãÂà•„ÄÇ\\n\\n```python\\n# Êï¥Êï∏\\nx = 10\\n\\n# ÊµÆÈªûÊï∏\\ny = 3.14\\n\\n# Â≠ó‰∏≤\\nname = \"Alice\"\\n\\n# Â∏ÉÊûóÂÄº\\nis_active = True\\n```\\n\\n2. Âü∫Êú¨ÈÅãÁÆó\\n\\n   Python ÊîØÊè¥Â∏∏Ë¶ãÁöÑÊï∏Â≠∏ÈÅãÁÆóÔºå‰∏¶‰ΩøÁî®Á∞°ÂñÆÁöÑÁ¨¶Ëôü‰æÜË°®Á§∫„ÄÇ\\n\\n```python\\n# Âä†Ê≥ï\\nresult = 5 + 3 # 8\\n\\n# Ê∏õÊ≥ï\\nresult = 10 - 7 # 3\\n\\n# ‰πòÊ≥ï\\nresult = 4 \\\\* 2 # 8\\n\\n# Èô§Ê≥ï\\nresult = 10 / 2 # 5.0 (ÊµÆÈªûÊï∏ÁµêÊûú)\\n\\n# Ê¨°Êñπ\\nresult = 2 \\\\*\\\\* 3 # 8\\n```\\n\\n3. Ê¢ù‰ª∂Âà§Êñ∑\\n\\n   ‰ΩøÁî® if„ÄÅelif Âíå else ‰æÜÈÄ≤Ë°åÊ¢ù‰ª∂Âà§Êñ∑„ÄÇ\\n\\n```python\\nage = 18\\n\\nif age >= 18:\\nprint(\"Êàê‰∫∫\")\\nelif age > 12:\\nprint(\"ÈùíÂ∞ëÂπ¥\")\\nelse:\\nprint(\"ÂÖíÁ´•\")\\n```\\n\\n4. Ëø¥Âúà\\n\\n   Python ÊúâÂÖ©Á®Æ‰∏ªË¶ÅÁöÑËø¥ÂúàÔºöfor Âíå while„ÄÇ\\n\\n- for Ëø¥Âúà\\n\\n  ```python\\n  # ÂàóÂç∞ 1 Âà∞ 5\\n  for i in range(1, 6):\\n  print(i)\\n  ```\\n\\n- while Ëø¥Âúà\\n\\n  ```python\\n  # ÂàóÂç∞Áõ¥Âà∞Ê¢ù‰ª∂ÁÇ∫ÂÅá\\n  count = 0\\n  while count < 5:\\n  print(count)\\n  count += 1\\n  ```\\n\\n5. ÂáΩÂºè\\n\\n‰ΩøÁî® def ÈóúÈçµÂ≠ó‰æÜÂÆöÁæ©ÂáΩÂºè\\n\\n```python\\ndef greet(name):\\nreturn f\"Hello, {name}!\"\\n\\n# ÂëºÂè´ÂáΩÂºè\\nmessage = greet(\"Alice\")\\nprint(message) # Hello, Alice!\\n```\\n\\n6. Ê∏ÖÂñÆËàáËø≠‰ª£\\n\\n   Ê∏ÖÂñÆÊòØ Python ‰∏≠ÁöÑÂü∫Êú¨Ë≥áÊñôÁµêÊßãÔºåÂèØ‰ª•Â≠òÊîæÂ§öÂÄãÂÖÉÁ¥†„ÄÇ\\n\\n```python\\n# ÂÆöÁæ©Ê∏ÖÂñÆ\\nfruits = [\"ËòãÊûú\", \"È¶ôËïâ\", \"Ê©òÂ≠ê\"]\\n\\n# Ëø≠‰ª£Ê∏ÖÂñÆ\\nfor fruit in fruits:\\nprint(fruit)\\n```\\n\\n7. Â≠óÂÖ∏\\n\\n   Â≠óÂÖ∏ÊòØÁî±ÈçµÂÄºÂ∞çÁµÑÊàêÁöÑË≥áÊñôÁµêÊßã„ÄÇ\\n\\n```python\\n# ÂÆöÁæ©Â≠óÂÖ∏\\nperson = {\\n    \"name\": \"Alice\",\\n    \"age\": 25,\\n    \"city\": \"Taipei\"\\n}\\n\\n# Ë®™ÂïèÂ≠óÂÖ∏ÂÖÉÁ¥†\\nprint(person[\"name\"]) # Alice\\n```\\n\\n8. Áï∞Â∏∏ËôïÁêÜ\\n\\n   ‰ΩøÁî® try-except ‰æÜËôïÁêÜÂèØËÉΩÂá∫ÁèæÁöÑÈåØË™§„ÄÇ\\n\\n```python\\ntry:\\nresult = 10 / 0\\nexcept ZeroDivisionError:\\nprint(\"Èô§‰ª• 0 ÁöÑÈåØË™§ÔºÅ\")\\n```\\n\\nÈÄô‰∫õÊòØ Python ÁöÑ‰∏Ä‰∫õÂü∫Á§éË™ûÊ≥ïÔºåÈÅ©ÂêàÂàùÂ≠∏ËÄÖÂø´ÈÄüÂÖ•ÈñÄ„ÄÇÂ¶ÇÊûúË¶ÅÊ∑±ÂÖ•Â≠∏ÁøíÔºåÂèØ‰ª•ÈÄ≤‰∏ÄÊ≠•Êé¢Ë®éÁâ©‰ª∂Â∞éÂêë„ÄÅÊ®°ÁµÑËàáÂ•ó‰ª∂Á≠âÈÄ≤ÈöéÊ¶ÇÂøµ„ÄÇ'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].get_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Parser (SimpleFileNodeParser-No matter file type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    *SimpleDirectoryReader(input_dir=\"../data/json/\").load_data(),\n",
    "    *SimpleDirectoryReader(input_dir=\"../data/html/\").load_data(),\n",
    "    *SimpleDirectoryReader(input_dir=\"../data/markdown/\").load_data()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleFileNodeParser\n",
    "parser = SimpleFileNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter (SenstenceSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(text=\"Âú®‰ªäÂπ¥ÂÇôÂèóÁüöÁõÆÁöÑ NBA Á∏ΩÂÜ†ËªçË≥Ω‰∏≠ÔºåÊ¥õÊùâÁ£ØÊπñ‰∫∫Èöä‰ª•È©öÈö™ÁöÑË°®ÁèæÊìäÊïó‰∫ÜÊ≥¢Â£´È†ìÂ°ûÁàæÊèêÂÖãÈöäÔºå\\\n",
    "        ÊàêÂäüÊçßËµ∑ÈöäÂè≤Á¨¨ 18 Â∫ßÁ∏ΩÂÜ†ËªçÁçéÊùØÔºåÂà∑Êñ∞ËÅØÁõüÊ≠∑Âè≤Ë®òÈåÑ„ÄÇÁ≥ªÂàóË≥Ω‰∏≠ÔºåLeBron James Âíå Anthony Davis \\\n",
    "        ÊåÅÁ∫åÂ±ïÁèæÈ†ÇÂ∞ñË°®ÁèæÔºåÂ∞§ÂÖ∂Âú®ÈóúÈçµÁöÑÁ¨¨‰∏ÉÂ†¥ÊØîË≥Ω‰∏≠ÔºåLeBron Âú®ÊúÄÂæå‰∏âÁßíÊäï‰∏≠Ëá¥ÂãùÁêÉÔºåÊàêÁÇ∫ÁêÉÈöäÂ•™ÂÜ†ÁöÑÊúÄÂ§ßÂäüËá£„ÄÇ\\\n",
    "        ÂÑòÁÆ°Â°ûÁàæÊèêÂÖãÈöäÁöÑÂπ¥ËºïÊ†∏ÂøÉ Jayson Tatum Âíå Jaylen Brown Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÊúÄÁµÇÁÑ°Ê≥ïÈÄÜËΩâÊπñ‰∫∫ÈöäÁöÑ\\\n",
    "        ÂÖ®Èù¢ÂÑ™Âã¢„ÄÇÈÄôÂ†¥Âè≤Ë©©Ëà¨ÁöÑÂ∞çÊ±∫‰∏çÂÉÖÂ±ïÁèæ‰∫ÜÈõôÊñπÁêÉÂì°ÁöÑÊäÄË°ìËàáÈüåÊÄßÔºå‰πüÁÇ∫ÁêÉËø∑Áïô‰∏ã‰∫ÜÁÑ°Êï∏Á∂ìÂÖ∏Áû¨ÈñìÔºå\\\n",
    "        ÊàêÁÇ∫ NBA Ê≠∑Âè≤‰∏äÁöÑÂèà‰∏ÄÈáåÁ®ãÁ¢ë\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# Default chunk_size is 1024, chunk_overlap is 200\n",
    "# Be aware that the chunk_overlap cannot larger than chunk_size\n",
    "splitter = SentenceSplitter(chunk_size=100, chunk_overlap=20)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Âú®‰ªäÂπ¥ÂÇôÂèóÁüöÁõÆÁöÑ NBA Á∏ΩÂÜ†ËªçË≥Ω‰∏≠ÔºåÊ¥õÊùâÁ£ØÊπñ‰∫∫Èöä‰ª•È©öÈö™ÁöÑË°®ÁèæÊìäÊïó‰∫ÜÊ≥¢Â£´È†ìÂ°ûÁàæÊèêÂÖãÈöäÔºå        ÊàêÂäüÊçßËµ∑ÈöäÂè≤Á¨¨ 18'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].get_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually create node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='560ba749-c35f-4736-92f2-b74f46d75b31', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='1th chunk', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='f1209f8c-9d51-4200-a9c6-4ee359bfc1ed', embedding=None, metadata={'kind': 'special chunk'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2nd chunk', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='13873cec-1735-4973-bc27-31ab1b28708b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='3rd chunk', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "nodes = [\n",
    "    TextNode(text=\"1th chunk\"),\n",
    "    TextNode(text=\"2nd chunk\", metadata={\"kind\": \"special chunk\"}),\n",
    "    TextNode(text=\"3rd chunk\", id=3),\n",
    "]\n",
    "nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29b0e7fac241db46251d42ccda13599930addaa78f589d657e65b2641aeb9779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
